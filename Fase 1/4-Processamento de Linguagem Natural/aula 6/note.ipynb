{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "artigo_treino = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Word2Vec/treino.csv\")\n",
    "artigo_teste = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Word2Vec/teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m SpaCy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SpaCy\n",
    "nlp = SpaCy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_para_tratamento = (titulos.lower() for titulos in artigo_treino.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_textos(doc):\n",
    "  tokens_validos = []\n",
    "\n",
    "\n",
    "  for token in doc:\n",
    "    e_valido = not token.is_stop and token.is_alpha\n",
    "    if e_valido:\n",
    "      tokens_validos.append(token.text)\n",
    "\n",
    "\n",
    "  if len(tokens_validos) > 2:\n",
    "    return \" \".join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_tratados = [trata_textos(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
    "  batch_size = 1000,\n",
    "  n_process = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo = Word2Vec(sg = 0,\n",
    "                      window = 2,\n",
    "                      vector_size = 300,\n",
    "                      min_count = 5,\n",
    "                      alpha = 0.03,\n",
    "                      min_alpha = 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_lista_tokens = [titulo.split(\" \") for titulo in titulos_tratados.titulo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.build_vocab(lista_lista_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# iniciando a chamada callback\n",
    "class callback(CallbackAny2Vec):\n",
    "  def __init__(self):\n",
    "    self.epoch = 0\n",
    "\n",
    "  def on_epoch_end(self, model):\n",
    "    loss = model.get_latest_training_loss()\n",
    "    if self.epoch == 0:\n",
    "        print('Loss após a época {}: {}'.format(self.epoch, loss))\n",
    "    else:\n",
    "        print('Loss após a época {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "    self.epoch += 1\n",
    "    self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.train(lista_lista_tokens,\n",
    "  total_examples = w2v_modelo.corpus_count,\n",
    "  epochs = 30,\n",
    "  compute_loss = True,\n",
    "  callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo_sg = Word2Vec(sg = 1,\n",
    "  window = 5,\n",
    "  vector_size = 300,\n",
    "  min_count = 5,\n",
    "  alpha = 0.03,\n",
    "  min_alpha = 0.007)\n",
    "\n",
    "w2v_modelo_sg.build_vocab(lista_lista_tokens)\n",
    "\n",
    "w2v_modelo_sg.train(lista_lista_tokens,\n",
    "total_examples = w2v_modelo_sg.corpus_count,\n",
    "epochs = 30,\n",
    "compute_loss = True,\n",
    "callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.save_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/Word2Vec/modelo_cbow.txt\", binary=False)\n",
    "w2v_modelo_sg.wv.save_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/Word2Vec/modelo_sg.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SpaCy\n",
    "nlp = SpaCy.load(\"pt_core_news_sm\", disable=[\"parser\", \"ner\", \"tagger\", \"textcat\"])\n",
    "\n",
    "def tokenizador(texto):\n",
    "  tokens_validos = []\n",
    "  doc = nlp(texto)\n",
    "\n",
    "  for token in doc:\n",
    "    e_valido = not token.is_stop and token.is_alpha\n",
    "    if e_valido:\n",
    "      tokens_validos.append(token.text.lower())\n",
    "\n",
    "  return tokens_validos\n",
    "\n",
    "texto = \"Adoro a 342342 #$@#@#$ cidade de caldas novas!\"\n",
    "tokenizador(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def combinacao_de_vetores_por_soma(palavras, modelo):\n",
    "  vetor_resultante = np.zeros(300)\n",
    "  for pn in palavras:\n",
    "    try:\n",
    "      vetor_resultante =+ modelo.wv.get_vector(pn)\n",
    "    except KeyError:\n",
    "      pass\n",
    "\n",
    "  return vetor_resultante\n",
    "\n",
    "def matriz_vetores(textos, modelo):\n",
    "  x = len(textos)\n",
    "  y = 300\n",
    "  matriz = np.zeros((x,y))\n",
    "\n",
    "  for i in range(x):\n",
    "    palavras = tokenizador(textos.iloc[i])\n",
    "    matriz[i] = combinacao_de_vetores_por_soma(palavras, modelo)\n",
    "\n",
    "  return matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_vetores_treino_cbow = matriz_vetores(artigo_treino.title, w2v_modelo)\n",
    "matriz_vetores_teste_cbow = matriz_vetores(artigo_teste.title, w2v_modelo)\n",
    "print(matriz_vetores_treino_cbow.shape)\n",
    "print(matriz_vetores_teste_cbow.shape)\n",
    "\n",
    "matriz_vetores_treino_sg = matriz_vetores(artigo_treino.title, w2v_modelo_sg)\n",
    "matriz_vetores_teste_sg = matriz_vetores(artigo_teste.title, w2v_modelo_sg)\n",
    "print(matriz_vetores_treino_sg.shape)\n",
    "print(matriz_vetores_teste_sg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classificador(modelo, x_treino, y_treino, x_teste, y_teste):\n",
    "  RL = LogisticRegression(max_iter = 800)\n",
    "  RL.fit(x_treino, y_treino)\n",
    "  categorias = RL.predict(x_teste)\n",
    "  resultados = classification_report(y_teste, categorias)\n",
    "  print(resultados)\n",
    "  return RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_cbow = classificador(w2v_modelo,\n",
    "  matriz_vetores_treino_cbow,\n",
    "  artigo_treino.category,\n",
    "  matriz_vetores_teste_cbow,\n",
    "  artigo_teste.category)\n",
    "\n",
    "RL_cbow = classificador(w2v_modelo_sg,\n",
    "  matriz_vetores_treino_sg,\n",
    "  artigo_treino.category,\n",
    "  matriz_vetores_teste_sg,\n",
    "  artigo_teste.category)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
